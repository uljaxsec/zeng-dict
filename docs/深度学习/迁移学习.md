# 迁移学习
迁移学习（Transfer Learning）是指将一个领域的知识迁移到另一个领域。在深度学习领域中，迁移学习指的是在某些任务中预训练一个模型，并将这个模型的参数作为另一个任务的初始化参数，以加速模型的训练过程，并提高模型的泛化能力。

一般来说，深度学习需要大量的数据才能训练出一个好的模型。但在某些应用场景中，很难得到充足的训练数据。这时，迁移学习可以很好地解决这个问题。尤其是在计算机视觉领域中，迁移学习已经变得格外重要。

例如，我们在解决一个手写数字识别问题时，已经有了一个训练好的卷积神经网络（CNN）用于识别 0 到 9 这一范围内的数字。在一个新的场景下，我们需要识别 10 到 19 的数字。这时可以直接使用之前训练好的 CNN，但是需要在已有的 CNN 的基础上进行微调。

常用的迁移学习方法有以下几种：

1. 全局微调：在预训练模型的基础上，在新数据集上进行微调。
2. 特征提取：将预训练模型的前几层作为特征提取器，将提取的特征作为新模型的输入，然后在新数据集上进行训练。
3. 网络套用：将预训练模型的参数套用到新的网络结构中，通常用于解决识别相关问题，比如在解决人脸识别问题时。

使用迁移学习可以大大提高模型的泛化能力和准确性，减少参数量，并且加速模型的训练速度。